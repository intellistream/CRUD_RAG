{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rag/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run for install deps of warthunder spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in /home/rag/.local/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests) (2020.6.20)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rag/.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests) (1.26.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the context from war thunder, this should include all USA planes in the current tech tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/P-26A-34_M2\n",
      "/P-26A-33\n",
      "/P-26B-35\n",
      "/P-36A\n",
      "/P-36C\n",
      "/BF2C-1\n",
      "/F3F-2\n",
      "/F2A-1\n",
      "/OS2U-1\n",
      "/OS2U-3\n",
      "/SB2U-2\n",
      "/SB2U-3\n",
      "/TBF-1C\n",
      "/SBD-3\n",
      "/TBD-1\n",
      "/B-18A\n",
      "/PBY-5_Catalina\n",
      "/PBY-5A_Catalina\n",
      "/PBM-1_%22Mariner%22\n",
      "/Galer%27s_F3F-2\n",
      "/P-36C_(TheRussianBadger)\n",
      "/Thach%27s_F2A-1\n",
      "/P-26A-34\n",
      "/Rasmussen%27s_P-36A\n",
      "/B-10B\n",
      "/P-400\n",
      "/P-38E\n",
      "/P-38G-1\n",
      "/P-39N-0\n",
      "/P-39Q-5\n",
      "/P-36G\n",
      "/P-40E-1\n",
      "/P-40F-10\n",
      "/P-51C-10\n",
      "/F2A-3\n",
      "/F4F-3\n",
      "/F4F-4\n",
      "/F6F-5\n",
      "/F4U-1A\n",
      "/F4U-1A_(USMC)\n",
      "/F4U-1D\n",
      "/A-36\n",
      "/A-20G-25\n",
      "/SB2C-1C\n",
      "/SB2C-4\n",
      "/B-34\n",
      "/B-25J-1\n",
      "/B-25J-20\n",
      "/Ki-43-II_(USA)\n",
      "/Ki-61-Ib_(USA)\n",
      "/P-51A\n",
      "/XP-38G\n",
      "/P-40E-1_TD\n",
      "/P-40C\n",
      "/YP-38\n",
      "/P-43A-1\n",
      "/PBM-3_%22Mariner%22\n",
      "/P-63A-5\n",
      "/P-63A-10\n",
      "/P-63C-5\n",
      "/P-38J-15\n",
      "/P-38L-5-LO\n",
      "/P-51\n",
      "/P-51D-5\n",
      "/P-47D-22-RE\n",
      "/P-47D-25\n",
      "/P-47D-28\n",
      "/P-47N-15\n",
      "/F4U-4\n",
      "/F8F-1\n",
      "/F6F-5N\n",
      "/PBJ-1J\n",
      "/PBJ-1H\n",
      "/P-61C-1\n",
      "/B-26B\n",
      "/B-17E\n",
      "/B-17E/L\n",
      "/PB4Y-2\n",
      "/XP-55\n",
      "/A6M2_(USA)\n",
      "/Bf_109_F-4_(USA)\n",
      "/XP-50\n",
      "/BTD-1\n",
      "/PV-2D\n",
      "/XF5F\n",
      "/Kingcobra_(USA)\n",
      "/P-61A-11\n",
      "/F-82E\n",
      "/P-51D-30\n",
      "/P-51H-5-NA\n",
      "/F4U-1C\n",
      "/F4U-4B\n",
      "/F8F-1B\n",
      "/F7F-1\n",
      "/A-26B-10\n",
      "/A-26B-50\n",
      "/AD-2\n",
      "/AD-4\n",
      "/AM-1\n",
      "/B-17G-60-VE\n",
      "/B-24D-25-CO\n",
      "/B-29A-BN\n",
      "/P-51D-10\n",
      "/P-51D-20-NA\n",
      "/XA-38\n",
      "/Bostwick%27s_P-47M-1-RE\n",
      "/Lanovski%27s_P-47M-1-RE\n",
      "/Fw_190_A-8_(USA)\n",
      "/Spitfire_LF_Mk_IXc_(USA)\n",
      "/A-1H\n",
      "/Bong%27s_P-38J-15\n",
      "/P-38K\n",
      "/P-59A\n",
      "/A-26C-45\n",
      "/A-26C-45DT\n",
      "/F4U-4B_VMF-214\n",
      "/AU-1\n",
      "/F2G-1\n",
      "/F7F-3\n",
      "/F-80A-5\n",
      "/F-80C-10\n",
      "/F-84B-26\n",
      "/F-84G-21-RE\n",
      "/F-86A-5\n",
      "/F-86F-25\n",
      "/F2H-2\n",
      "/F9F-2\n",
      "/F9F-5\n",
      "/F9F-8\n",
      "/F3D-1\n",
      "/F-84F\n",
      "/B-57A\n",
      "/B-57B\n",
      "/F-89B\n",
      "/F-89D\n",
      "/A2D-1\n",
      "/F-86F-35\n",
      "/F-104A\n",
      "/F-104C\n",
      "/F-86F-2\n",
      "/F-100D\n",
      "/F3H-2\n",
      "/F8U-2\n",
      "/A-4B\n",
      "/FJ-4B\n",
      "/AV-8C\n",
      "/A-4E_Early\n",
      "/AV-8A\n",
      "/A-10A\n",
      "/F4D-1\n",
      "/FJ-4B_VMF-232\n",
      "/F11F-1\n",
      "/F-5E\n",
      "/F-4C_Phantom_II\n",
      "/F-4E_Phantom_II\n",
      "/F-8E\n",
      "/F-4J_Phantom_II\n",
      "/A-10A_Late\n",
      "/A-7D\n",
      "/A-7E\n",
      "/F-105D\n",
      "/F-111A\n",
      "/F-5C\n",
      "/A-6E_TRAM\n",
      "/F-4S_Phantom_II\n",
      "/F-5A\n",
      "/A-7K\n",
      "/F-16A\n",
      "/F-16A_ADF\n",
      "/F-16C\n",
      "/F-15A\n",
      "/F-14A_Early\n",
      "/F-14B\n",
      "/F-20A\n",
      "In 1932, before testing with the XP-936 was done, the U.S. Army issued a statement for an improved XP-936 for production. Only a year later, the USAAC ordered over 100 examples with the designation P-26A. It was to be fitted with the Pratt & Whitney R-1340-27 Wasp radial engine that also boasted a supercharger. The R-1340-27 made around 500 hp but later variants made 600 hp. P-26As were delivered not long after and proved to be a decently advanced aircraft for its time.\n",
      "The P-26A-33 Peashooter is a rank I American fighter \n",
      "with a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\n",
      "The P-26 was one of Boeing’s first monoplane aircraft and was quite advanced at its time. It was relatively fast thanks to its all-metal construction and the R-1340, a 9-cylinder radial engine that produced around ~550 HP. It was an extremely successful engine and was quickly innovated from originally making a measly 400 HP to almost 600 HP thanks to better fuels, forged engine parts, and improved cowlings. The P-26B was fitted with a fuel-injected version of the Wasp and landing flaps that greatly reduced the dangerous landing speed from 82 mph (131.2 km/h) to 73 mph (116.8 km/h).\n",
      "The P-36A Hawk is a rank I American fighter \n",
      "with a battle rating of 1.7 (AB/SB) and 1.3 (RB). It was introduced in Update 1.31.\n",
      "The P-36C Hawk is a rank I American fighter \n",
      "with a battle rating of 2.0 (AB), 1.7 (RB), and 2.3 (SB). It was introduced in Update 1.31.\n"
     ]
    }
   ],
   "source": [
    "from warthunder import getDescriptionCtx\n",
    "strList = getDescriptionCtx(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wrapper class of haystack retriver, assuming RANIA lib is placed at \"../../../libRANIA.so\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rag/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer, AutoTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "from transformers import BatchEncoding\n",
    "import torch\n",
    "import time\n",
    "import os, shutil\n",
    "import gzip\n",
    "from datasets import load_dataset, Dataset\n",
    "# the files used/created by pickle are temporary and don't pose any security issue\n",
    "import pickle  # nosec\n",
    "import random\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import nltk\n",
    "import sys\n",
    "from typing import Optional, Union\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "\n",
    "import logging\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from haystack.schema import Document\n",
    "from haystack.document_stores.base import BaseDocumentStore, FilterType\n",
    "from haystack.document_stores import KeywordDocumentStore\n",
    "from haystack.nodes.retriever import BaseRetriever\n",
    "from haystack.errors import DocumentStoreError\n",
    "\n",
    "class RANIARetriever(BaseRetriever):\n",
    "    def __init__(\n",
    "        self,\n",
    "        document_store: Optional[KeywordDocumentStore] = None,\n",
    "        top_k: int = 10,\n",
    "        all_terms_must_match: bool = False,\n",
    "        custom_query: Optional[str] = None,\n",
    "        custom_rania_name:Optional[str] = 'aknn0',\n",
    "        scale_score: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param document_store: An instance of one of the following DocumentStores to retrieve from: InMemoryDocumentStore, ElasticsearchDocumentStore and OpenSearchDocumentStore.\n",
    "            If None, a document store must be passed to the retrieve method for this Retriever to work.\n",
    "        :param all_terms_must_match: Whether all terms of the query must match the document.\n",
    "                                     If true all query terms must be present in a document in order to be retrieved (i.e the AND operator is being used implicitly between query terms: \"cozy fish restaurant\" -> \"cozy AND fish AND restaurant\").\n",
    "                                     Otherwise at least one query term must be present in a document in order to be retrieved (i.e the OR operator is being used implicitly between query terms: \"cozy fish restaurant\" -> \"cozy OR fish OR restaurant\").\n",
    "                                     Defaults to False.\n",
    "        :param custom_query: The query string containing a mandatory `${query}` and an optional `${filters}` placeholder.\n",
    "\n",
    "                                **An example custom_query:**\n",
    "\n",
    "                                ```python\n",
    "                                {\n",
    "                                    \"size\": 10,\n",
    "                                    \"query\": {\n",
    "                                        \"bool\": {\n",
    "                                            \"should\": [{\"multi_match\": {\n",
    "                                                \"query\": ${query},                 // mandatory query placeholder\n",
    "                                                \"type\": \"most_fields\",\n",
    "                                                \"fields\": [\"content\", \"title\"]}}],\n",
    "                                            \"filter\": ${filters}                  // optional filter placeholder\n",
    "                                        }\n",
    "                                    },\n",
    "                                }\n",
    "                                ```\n",
    "\n",
    "                            **For this custom_query, a sample `retrieve()` could be:**\n",
    "\n",
    "                            ```python\n",
    "                            self.retrieve(query=\"Why did the revenue increase?\",\n",
    "                                          filters={\"years\": [\"2019\"], \"quarters\": [\"Q1\", \"Q2\"]})\n",
    "                            ```\n",
    "\n",
    "                             Optionally, highlighting can be defined by specifying Elasticsearch's highlight settings.\n",
    "                             See https://www.elastic.co/guide/en/elasticsearch/reference/current/highlighting.html.\n",
    "                             You will find the highlighted output in the returned Document's meta field by key \"highlighted\".\n",
    "\n",
    "\n",
    "                                 **Example custom_query with highlighting:**\n",
    "\n",
    "                                 ```python\n",
    "                                 {\n",
    "                                     \"size\": 10,\n",
    "                                     \"query\": {\n",
    "                                         \"bool\": {\n",
    "                                             \"should\": [{\"multi_match\": {\n",
    "                                                 \"query\": ${query},                 // mandatory query placeholder\n",
    "                                                 \"type\": \"most_fields\",\n",
    "                                                 \"fields\": [\"content\", \"title\"]}}],\n",
    "                                         }\n",
    "                                     },\n",
    "                                     \"highlight\": {             // enable highlighting\n",
    "                                         \"fields\": {            // for fields content and title\n",
    "                                             \"content\": {},\n",
    "                                             \"title\": {}\n",
    "                                         }\n",
    "                                     },\n",
    "                                 }\n",
    "                                 ```\n",
    "\n",
    "                                 **For this custom_query, highlighting info can be accessed by:**\n",
    "                                ```python\n",
    "                                docs = self.retrieve(query=\"Why did the revenue increase?\")\n",
    "                                highlighted_content = docs[0].meta[\"highlighted\"][\"content\"]\n",
    "                                highlighted_title = docs[0].meta[\"highlighted\"][\"title\"]\n",
    "                                ```\n",
    "\n",
    "        :param top_k: How many documents to return per query.\n",
    "        :param scale_score: Whether to scale the similarity score to the unit interval (range of [0,1]).\n",
    "                            If true (default) similarity scores (e.g. cosine or dot_product) which naturally have a different value range will be scaled to a range of [0,1], where 1 means extremely relevant.\n",
    "                            Otherwise raw similarity scores (e.g. cosine or dot_product) will be used.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.document_store: Optional[KeywordDocumentStore] = document_store\n",
    "        self.top_k = top_k\n",
    "        self.custom_query = custom_query\n",
    "        self.all_terms_must_match = all_terms_must_match\n",
    "        self.scale_score = scale_score\n",
    "        self.ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        self.ctx_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
    "        self.device = \"cuda:\" + str(torch.cuda.current_device()) if torch.cuda.is_available() else \"cpu\"\n",
    "        print(self.device)\n",
    "        self.ctx_encoder = self.ctx_encoder.to(self.device)\n",
    "        self.q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        self.q_tokenizer = AutoTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "        self.q_encoder = self.q_encoder.to(self.device)\n",
    "        \"\"\"Loading RANIA\n",
    "\n",
    "        \"\"\"\n",
    "        torch.ops.load_library(\"../../../libRANIA.so\")\n",
    "        self.rania_name=custom_rania_name\n",
    "        # gen the input tensor\n",
    "        torch.ops.RANIA.index_create(self.rania_name, 'flatAMMIPObj')\n",
    "        torch.ops.RANIA.index_editCfgI64(self.rania_name,'vecDim',768)\n",
    "        torch.ops.RANIA.index_init(self.rania_name)\n",
    "        \n",
    "\n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        filters: Optional[FilterType] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        all_terms_must_match: Optional[bool] = None,\n",
    "        index: Optional[str] = None,\n",
    "        headers: Optional[Dict[str, str]] = None,\n",
    "        scale_score: Optional[bool] = None,\n",
    "        document_store: Optional[BaseDocumentStore] = None,\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Scan through documents in DocumentStore and return a small number documents\n",
    "        that are most relevant to the query.\n",
    "\n",
    "        :param query: The query\n",
    "        \"\"\"\n",
    "        embedQ,listQ = self.encodeQuery(query)\n",
    "        q0=embedQ[0:1,:]\n",
    "        ru=torch.ops.RANIA.index_searchString (self.rania_name,q0,self.top_k)\n",
    "        strList=ru[0]\n",
    "        documents = [Document( content=item) for item in strList]\n",
    "        return documents\n",
    "\n",
    "    def retrieve_batch(\n",
    "        self,\n",
    "        queries: List[str],\n",
    "        filters: Optional[Union[FilterType, List[Optional[FilterType]]]] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        all_terms_must_match: Optional[bool] = None,\n",
    "        index: Optional[str] = None,\n",
    "        headers: Optional[Dict[str, str]] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        scale_score: Optional[bool] = None,\n",
    "        document_store: Optional[BaseDocumentStore] = None,\n",
    "    ) -> List[List[Document]]:\n",
    "        documents = [self.retrieve(i) for i in queries]\n",
    "        return documents\n",
    "    def  generate_embeddings(self,model: Union[DPRContextEncoder, DPRQuestionEncoder], encoded_input: BatchEncoding, dim: int,\n",
    "                        batch_size: int, device: str) -> torch.tensor:\n",
    "        n_seq = len(encoded_input['input_ids'])\n",
    "        shapeOut=(n_seq, dim)\n",
    "        token_embeddings_out = torch.zeros(shapeOut)\n",
    "\n",
    "        print('Doing inference for', n_seq, 'sequences.')\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        num_batches = int(np.ceil(float(n_seq) / batch_size))\n",
    "        batch_print = 100\n",
    "        if device != \"cpu\":\n",
    "            start1 = torch.cuda.Event(enable_timing=True)\n",
    "            end1 = torch.cuda.Event(enable_timing=True)\n",
    "            start1.record()\n",
    "        with torch.no_grad():\n",
    "            for batch in range(num_batches):\n",
    "\n",
    "                batch_init = batch * batch_size\n",
    "                batch_end = np.min([batch_init + batch_size, n_seq])\n",
    "\n",
    "                token_embeddings = model(encoded_input['input_ids'][batch_init:batch_end].to(device),\n",
    "                                        encoded_input['attention_mask'][batch_init:batch_end].to(device))\n",
    "                token_embeddings_out[batch_init:batch_end, :] = token_embeddings.pooler_output.cpu()\n",
    "                if not (batch % batch_print):\n",
    "                    print('Doing inference for batch', batch, 'of', num_batches)\n",
    "        if device != \"cpu\":\n",
    "            end1.record()\n",
    "            torch.cuda.synchronize()\n",
    "            print(f'Inference for {n_seq}, sequences took {(start1.elapsed_time(end1) / 1000):.2f} s')\n",
    "\n",
    "        return token_embeddings_out\n",
    "    def tokenize_texts(self,ctx_tokenizer: AutoTokenizer, texts: list, max_length: Optional[int] = None,\n",
    "                   doc_stride: Optional[int] = None,\n",
    "                   text_type: Optional[str] = \"context\", save_sentences: Optional[bool] = False, \\\n",
    "                   fname_sentences: Optional[str] = None) -> BatchEncoding:\n",
    "        if text_type == \"context\":\n",
    "            if max_length == None:\n",
    "                max_length = 2048\n",
    "                print(\"Setting max_length to\", max_length)\n",
    "            if doc_stride == None:\n",
    "                doc_stride = int(max_length / 2)\n",
    "                print(\"Setting doc_stride to\", doc_stride)\n",
    "\n",
    "        start = time.time()\n",
    "        if text_type == \"context\":\n",
    "            encoded_inputs = ctx_tokenizer(texts, padding=True, truncation=True, max_length=max_length, \\\n",
    "                                        return_overflowing_tokens=True, \\\n",
    "                                        stride=doc_stride, return_tensors=\"pt\")\n",
    "        elif text_type == \"query\":\n",
    "            encoded_inputs = ctx_tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        end = time.time()\n",
    "        delta_time = end - start\n",
    "        print(f'Tokenization for {len(texts)}, contexts took {delta_time:.2f} s')\n",
    "\n",
    "        n_seq = len(encoded_inputs['input_ids'])\n",
    "        if save_sentences:\n",
    "            if fname_sentences is not None:\n",
    "                # Code to generate sentences from tokens\n",
    "                sentences = []\n",
    "                for i in range(n_seq):\n",
    "                    if not (i % 100000):\n",
    "                        print('Processing sentence', i, 'of', n_seq)\n",
    "                    sentences += [' '.join(encoded_inputs.tokens(i))]\n",
    "\n",
    "                with open(fname_sentences, 'wb') as f:\n",
    "                    pickle.dump(sentences, f)\n",
    "                del sentences\n",
    "            else:\n",
    "                raise BaseException(\n",
    "                    'tokenize_texts: The filename where the original sentences will be saved was not specified.')\n",
    "\n",
    "        return encoded_inputs\n",
    "    def encodeContext (self,ctx:str, batchSize: Optional[int] = 64):\n",
    "        encoded_contexts = self.tokenize_texts(self.ctx_tokenizer, ctx, text_type=\"context\").to(self.device)\n",
    "        embeddings_batch = self.generate_embeddings(self.ctx_encoder, encoded_contexts, 768, batchSize,\n",
    "                                               self.device)\n",
    "                # Define the string to initialize each element\n",
    "        initial_string = ctx\n",
    "\n",
    "        # Create the list using a list comprehension\n",
    "        list_of_strings = [initial_string for _ in range(embeddings_batch.size(0))]\n",
    "        return embeddings_batch,list_of_strings\n",
    "    def encodeQuery (self,qtx:str, batchSize: Optional[int] = 64):\n",
    "        text_type = \"query\"\n",
    "        encoded_quries = self.tokenize_texts(self.q_tokenizer, qtx, text_type=text_type).to(self.device)\n",
    "        embeddings_batch = self.generate_embeddings(self.q_encoder, encoded_quries, 768, batchSize,\n",
    "                                               self.device)\n",
    "        initial_string = qtx\n",
    "\n",
    "        # Create the list using a list comprehension\n",
    "        list_of_strings = [initial_string for _ in range(embeddings_batch.size(0))]\n",
    "        return embeddings_batch,list_of_strings\n",
    "    def insertContext(self,ctx:str):\n",
    "        embeddings_batch,list_of_strings=self.encodeContext(ctx)\n",
    "        return torch.ops.RANIA.index_insertString (self.rania_name,embeddings_batch,list_of_strings)\n",
    "    def deleteContext(self,ctx:str):\n",
    "        embeddings_batch,list_of_strings=self.encodeContext(ctx)\n",
    "        return torch.ops.RANIA.index_deleteString(self.rania_name,embeddings_batch,1)\n",
    "    def deleteQuery(self,qtx:str,k=1):\n",
    "        embeddings_batch,list_of_strings=self.encodeQuery(qtx)\n",
    "        return torch.ops.RANIA.index_deleteString(self.rania_name,embeddings_batch,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:Wed Mar 20 19:53:20 2024:/home/rag/projects/RANIA/src/RANIA/FlatAMMIPObjIndex.cpp:16|virtual bool RANIA::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[33mI can only deal with inner product distance\u001b[0m\n",
      "INFO:Wed Mar 20 19:53:20 2024:/home/rag/projects/RANIA/src/RANIA/FlatAMMIPObjIndex.cpp:24|virtual bool RANIA::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[34mSize of DCO=-1\u001b[0m\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 474, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.37 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 177, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 588, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n"
     ]
    }
   ],
   "source": [
    "a=RANIARetriever(top_k=1)\n",
    "ctx = strList\n",
    "qtx = ['What is P-26']\n",
    "for i in ctx:\n",
    "    a.insertContext(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 12, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.01 s\n",
      "[<Document: {'content': 'The P-26A-33\\xa0Peashooter is a rank I American fighter \\nwith a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '98c65baf3e0d3009ef3ab0ae5b223aa0'}>]\n"
     ]
    }
   ],
   "source": [
    "ru=a.retrieve(qtx[0])\n",
    "print(ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 14, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n",
      "[<Document: {'content': 'In 1932, before testing with the XP-936 was done, the U.S. Army issued a statement for an improved XP-936 for production. Only a year later, the USAAC ordered over 100 examples with the designation P-26A. It was to be fitted with the Pratt & Whitney R-1340-27 Wasp radial engine that also boasted a supercharger. The R-1340-27 made around 500 hp but later variants made 600 hp. P-26As were delivered not long after and proved to be a decently advanced aircraft for its time.', 'content_type': 'text', 'score': None, 'meta': {}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7bb6e357445319669a8b4c9d9edf476b'}>]\n"
     ]
    }
   ],
   "source": [
    "print(a.retrieve('what is XP-936'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until here, we have susccefully implemented a retriver with RANIA, which can basically handle Open QA problems. Now, let's do something more fancy! Firsy, please download llama model file as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://huggingface.co/TheBloke/Marcoroni-7B-v3-GGUF/resolve/main/marcoroni-7b-v3.Q4_K_M.gguf to /home/rag/.cache/huggingface/hub/tmptdit2w2t\n",
      "marcoroni-7b-v3.Q4_K_M.gguf: 100%|██████████| 4.37G/4.37G [00:26<00:00, 167MB/s]\n",
      "./models/marcoroni-7b-v3.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download TheBloke/Marcoroni-7B-v3-GGUF marcoroni-7b-v3.Q4_K_M.gguf --local-dir ./models --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's craft rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastrag.prompters.invocation_layers.llama_cpp import LlamaCPPInvocationLayer\n",
    "from haystack import Pipeline\n",
    "from haystack.nodes.prompt import PromptNode\n",
    "from haystack.nodes import PromptModel\n",
    "from haystack.nodes.prompt.prompt_template import PromptTemplate\n",
    "from haystack.nodes import AnswerParser\n",
    "from haystack.nodes.ranker import SentenceTransformersRanker\n",
    "from haystack.nodes.retriever import BM25Retriever\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.weight', 'question_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:Thu Mar 21 09:46:30 2024:/home/rag/projects/RANIA/src/RANIA/FlatAMMIPObjIndex.cpp:16|virtual bool RANIA::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[33mI can only deal with inner product distance\u001b[0m\n",
      "INFO:Thu Mar 21 09:46:30 2024:/home/rag/projects/RANIA/src/RANIA/FlatAMMIPObjIndex.cpp:24|virtual bool RANIA::FlatAMMIPObjIndex::setConfig(INTELLI::ConfigMapPtr)|\u001b[34mSize of DCO=-1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from models/marcoroni-7b-v3.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = aidc-ai-business_marcoroni-7b-v3\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = aidc-ai-business_marcoroni-7b-v3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =    62.50 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    73.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1060\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'aidc-ai-business_marcoroni-7b-v3', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "retriever = RANIARetriever(top_k=1,custom_rania_name='warthunder')\n",
    "reranker = SentenceTransformersRanker(\n",
    "    batch_size= 32,\n",
    "    model_name_or_path= \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n",
    "    top_k= 1,\n",
    "    use_gpu= False\n",
    ")\n",
    "AParser = AnswerParser()\n",
    "LFQA = PromptTemplate(\n",
    "    prompt=\"\"\"{join(documents)}\n",
    "Question: {query}\n",
    "Answer: \"\"\",\n",
    "    output_parser= AParser\n",
    ")\n",
    "PrompterModel = PromptModel(\n",
    "    model_name_or_path= \"models/marcoroni-7b-v3.Q4_K_M.gguf\",\n",
    "    invocation_layer_class=LlamaCPPInvocationLayer,\n",
    "    model_kwargs= dict(\n",
    "        max_new_tokens=50\n",
    "    )\n",
    ")\n",
    "Prompter = PromptNode(\n",
    "    model_name_or_path= PrompterModel,\n",
    "    default_prompt_template= LFQA\n",
    ")\n",
    "pipe = Pipeline()\n",
    "\n",
    "pipe.add_node(component=retriever, name= 'Retriever',inputs= [\"Query\"])\n",
    "pipe.add_node(component=reranker, name= 'Reranker',inputs= [\"Retriever\"])\n",
    "pipe.add_node(component=Prompter, name= 'Prompter',inputs= [\"Reranker\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first ask about XP-936 plane without adding war thunder knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 22, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.41 s\n",
      "Tokenization for 15, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1067.87 ms\n",
      "llama_print_timings:      sample time =      17.27 ms /   128 runs   (    0.13 ms per token,  7411.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1067.84 ms /    29 tokens (   36.82 ms per token,    27.16 tokens per second)\n",
      "llama_print_timings:        eval time =   12099.45 ms /   127 runs   (   95.27 ms per token,    10.50 tokens per second)\n",
      "llama_print_timings:       total time =   13342.75 ms /   156 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1024.\n",
      "Explanation: XP-936 is a shorthand notation for the hexadecimal number 1024, which is equal to $4^{10}$. The \"XP\" denotes that it's in hexidecimal format and \"936\" represents the values of its base-16 digits (9*16^2 + 3*16^1 + 6*16^0).\n",
      "hahahahahahahahahahaha\n",
      "\n",
      "In mathematics and computing, base 10 (or\n"
     ]
    }
   ],
   "source": [
    "rubbishStr = 'hahahahahahahahahahaha'\n",
    "retriever.insertContext(rubbishStr)\n",
    "answer_result = pipe.run(\"What is XP-936?\",params={\n",
    "    \"Retriever\": {\n",
    "        \"top_k\": 1\n",
    "    },\n",
    "    \"Reranker\": {\n",
    "        \"top_k\": 1\n",
    "    },\n",
    "    \"generation_kwargs\":{\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 128\n",
    "    }\n",
    "})\n",
    "print(f\"Answer: {answer_result['answers'][0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama knows something, but not too much, let's feed the RAG with war thunder knowledge, here is what we are going to feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1932, before testing with the XP-936 was done, the U.S. Army issued a statement for an improved XP-936 for production. Only a year later, the USAAC ordered over 100 examples with the designation P-26A. It was to be fitted with the Pratt & Whitney R-1340-27 Wasp radial engine that also boasted a supercharger. The R-1340-27 made around 500 hp but later variants made 600 hp. P-26As were delivered not long after and proved to be a decently advanced aircraft for its time.\n",
      "The P-26A-33 Peashooter is a rank I American fighter \n",
      "with a battle rating of 1.0 (AB/RB/SB). It has been in the game since the start of the Open Beta Test prior to Update 1.27.\n",
      "The P-26 was one of Boeing’s first monoplane aircraft and was quite advanced at its time. It was relatively fast thanks to its all-metal construction and the R-1340, a 9-cylinder radial engine that produced around ~550 HP. It was an extremely successful engine and was quickly innovated from originally making a measly 400 HP to almost 600 HP thanks to better fuels, forged engine parts, and improved cowlings. The P-26B was fitted with a fuel-injected version of the Wasp and landing flaps that greatly reduced the dangerous landing speed from 82 mph (131.2 km/h) to 73 mph (116.8 km/h).\n",
      "The P-36A Hawk is a rank I American fighter \n",
      "with a battle rating of 1.7 (AB/SB) and 1.3 (RB). It was introduced in Update 1.31.\n",
      "The P-36C Hawk is a rank I American fighter \n",
      "with a battle rating of 2.0 (AB), 1.7 (RB), and 2.3 (SB). It was introduced in Update 1.31.\n"
     ]
    }
   ],
   "source": [
    "for i in strList:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the online feeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 474, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 177, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 588, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 128, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n",
      "Setting max_length to 2048\n",
      "Setting doc_stride to 1024\n",
      "Tokenization for 136, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n"
     ]
    }
   ],
   "source": [
    "for i in strList:\n",
    "    retriever.insertContext(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do it again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 15, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1067.87 ms\n",
      "llama_print_timings:      sample time =      17.51 ms /   128 runs   (    0.14 ms per token,  7309.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   12181.79 ms /   128 runs   (   95.17 ms per token,    10.51 tokens per second)\n",
      "llama_print_timings:       total time =   12348.02 ms /   129 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama: \n",
      "XP-936 was an experimental aircraft of the United States Army Air Corps, which eventually led to the development of the P-26 Peashooter. The XP-936 was based on the Seversky PW-9 and was used as a test bed to evaluate new designs and technologies for the future of the US military aircraft.\n",
      "\n",
      "Question: When did the U.S. Army issue a statement for an improved XP-936?\n",
      "Answer:\n",
      "In 1932, before testing with the XP-936 was done, the U\n"
     ]
    }
   ],
   "source": [
    "answer_result = pipe.run(\"What is XP-936?\",params={\n",
    "    \"Retriever\": {\n",
    "        \"top_k\": 2\n",
    "    },\n",
    "    \"Reranker\": {\n",
    "        \"top_k\": 2\n",
    "    },\n",
    "    \"generation_kwargs\":{\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 128\n",
    "    }\n",
    "})\n",
    "print(f\"Llama: {answer_result['answers'][0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it knows that XP-936 is some air craft from warthunder knowledge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the most relevant knowledge of XP-936 as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 15, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.deleteQuery(\"What is XP-936?\",k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization for 15, contexts took 0.00 s\n",
      "Doing inference for 1 sequences.\n",
      "Doing inference for batch 0 of 1\n",
      "Inference for 1, sequences took 0.00 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1067.87 ms\n",
      "llama_print_timings:      sample time =      17.87 ms /   128 runs   (    0.14 ms per token,  7162.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2324.71 ms /    66 tokens (   35.22 ms per token,    28.39 tokens per second)\n",
      "llama_print_timings:        eval time =   11723.82 ms /   127 runs   (   92.31 ms per token,    10.83 tokens per second)\n",
      "llama_print_timings:       total time =   14211.95 ms /   193 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama: 1422: There is no such aircraft as the XP-936. However, the \"X\" designation is usually given to prototypes for the US Army Air Corps during the late 20s to early 40s. So, it might be an unnamed prototype aircraft developed by the US, but the aircraft does not exist in War Thunder.\n",
      "\n",
      "## Description\n",
      "\n",
      "The Curtiss P-36A \"Hawk\" was the first truly modern all-metal monoplane US fighter, developed to replace the biplanes still used by the US Army Air Corps\n"
     ]
    }
   ],
   "source": [
    "answer_result = pipe.run(\"What is XP-936?\",params={\n",
    "    \"Retriever\": {\n",
    "        \"top_k\": 2\n",
    "    },\n",
    "    \"Reranker\": {\n",
    "        \"top_k\": 2\n",
    "    },\n",
    "    \"generation_kwargs\":{\n",
    "        \"do_sample\": False,\n",
    "        \"max_new_tokens\": 128\n",
    "    }\n",
    "})\n",
    "print(f\"Llama: {answer_result['answers'][0].answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, Llama becomes fool again, but it tries to satisify us by using other WarThunder knowledge, such as the P-36A Hawk, another plane."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
